import sqlite3
import pandas as pd
import numpy as np
import math
import json
import yaml
import os
import time
from datetime import datetime, timedelta
from functools import lru_cache
from typing import List, Dict, Any, Optional

# --- KERNEL IMPORTS ---
from .domain_model import domain_mgr
from .ledger import ledger 
from .policy_engine import policy_engine

# --- PROFIT VALIDATOR (Graceful Load) ---
try:
    from .profit_validator import ProfitValidator
except ImportError:
    # Fallback if module is missing
    ProfitValidator = None

# --- DEBATE ENGINE (Graceful Load) ---
debate_engine = None
try:
    from .debate import debate_engine
except Exception:
    pass

class Orchestrator:
    """
    The High-Performance Decision Engine (v10.30 - Self-Correcting).
    
    KEY FEATURES:
    1. Unified Timeline: Blends History + Hypercube Forecasts.
    2. Explainability: Exposes Audit Logs (Glass Box).
    3. Accuracy Context: Aggregates WMAPE scores across the hierarchy.
    4. Tactical Hydration: Fast context loading for Replenishment/Pricing.
    """
    
    def __init__(self):
        self.db_path = domain_mgr.db_path
        self.REVIEW_PERIOD_DAYS = 7        
        
        # Load Strategy
        self.dna = self._load_dna_profile()
        if ProfitValidator:
            self.validator = ProfitValidator(self.dna)
        else:
            self.validator = None

    def _load_dna_profile(self) -> Dict:
        try:
            base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            config_path = os.path.join(base_dir, "config", "default_dna.yaml")
            if os.path.exists(config_path):
                with open(config_path, 'r') as f: return yaml.safe_load(f)
        except Exception: pass
        return {"physics": {"wacc_daily": 0.0003, "hurdle_rate_roi": 0.15}}

    # ==============================================================================
    # ðŸ§  INTELLIGENCE SERVING LAYER (THE GLASS BOX BRIDGE)
    # ==============================================================================

    def get_forecast_explanation(self, sku_id: str) -> Dict:
        """
        Reads the latest Audit Log generated by the ML Engine.
        Called by /ml/explain/{sku_id}
        """
        try:
            # We look for the file generated by ml_engine.run_demand_pipeline()
            # This decouples the Orchestrator from the ML runtime memory
            log_path = "audit_log_latest.json"
            if os.path.exists(log_path):
                with open(log_path, 'r') as f:
                    data = json.load(f)
                    # Inject SKU specific context if needed (future enhancement)
                    data['target_sku'] = sku_id
                    return data
            else:
                return {
                    "run_id": "N/A",
                    "generated_at": datetime.now().isoformat(),
                    "data_health": {"score": 0, "log": ["No audit log found. Run Pipeline first."]},
                    "model_transparency": {"tournament_scoreboard": {}},
                    "drivers": {}
                }
        except Exception as e:
            print(f"[ORCHESTRATOR] Error reading audit log: {e}")
            return {}

    def get_accuracy_context(self, node_id: str) -> Dict:
        """
        Reads the Accuracy Matrix generated by the ML Engine.
        Called by /ml/accuracy/{node_id}
        """
        try:
            matrix_path = "accuracy_matrix.json"
            if os.path.exists(matrix_path):
                with open(matrix_path, 'r') as f:
                    matrix = json.load(f)
                    
                # Format the response for the UI widget
                if node_id == "GLOBAL" or not node_id:
                    return {
                        "node_id": "GLOBAL",
                        "scores": matrix.get("GLOBAL", {}),
                        "children_scores": matrix.get("children", [])
                    }
                else:
                    # In a full graph, we would search recursively. 
                    # For v1, we check if the requested node is one of the children or fallback to global structure
                    return {
                        "node_id": node_id,
                        "scores": matrix.get("GLOBAL", {}), # Placeholder for node specific score
                        "children_scores": []
                    }
            else:
                return {"scores": {}, "children_scores": []}
        except Exception as e:
            print(f"[ORCHESTRATOR] Error reading accuracy matrix: {e}")
            return {}

    # ==============================================================================
    # âš¡ CORE 1: TACTICAL HYDRATION (For Replenishment/Pricing)
    # ==============================================================================
    def _hydrate_context(self, days_history=60):
        """
        Fetches 'Current State' context for tactical decision engines.
        Uses Pandas to merge Product Attributes + Latest Events.
        """
        with sqlite3.connect(self.db_path) as conn:
            # 1. Load Catalog
            df_objs = pd.read_sql("SELECT obj_id, name, attributes FROM universal_objects WHERE obj_type='PRODUCT'", conn)
            
            def parse_attr(x):
                try: return json.loads(x)
                except: return {}
            
            # Vectorized Attribute Expansion
            attr_dicts = df_objs['attributes'].apply(parse_attr).tolist()
            df_attrs = pd.DataFrame(attr_dicts)
            df_full = pd.concat([df_objs, df_attrs], axis=1)
            
            # 2. Load Events (Recent History)
            cutoff = (datetime.now() - timedelta(days=days_history)).strftime("%Y-%m-%d")
            query = f"SELECT event_type, primary_target_id, value, timestamp, meta FROM universal_events WHERE timestamp >= '{cutoff}'"
            df_events = pd.read_sql(query, conn)
            
            # 3. Extract Latest State (Vectorized)
            if not df_events.empty:
                # Inventory (Latest WP or Actual)
                inv_mask = df_events['event_type'] == 'INV_SNAPSHOT'
                if inv_mask.any():
                    latest_inv = df_events[inv_mask].sort_values('timestamp').drop_duplicates('primary_target_id', keep='last')
                    df_full = df_full.merge(latest_inv[['primary_target_id', 'value']].rename(columns={'value': 'inventory'}), 
                                          left_on='obj_id', right_on='primary_target_id', how='left')
                
                # Demand (Mean Sales)
                sales_mask = df_events['event_type'] == 'SALES_QTY'
                if sales_mask.any():
                    demand = df_events[sales_mask].groupby('primary_target_id')['value'].agg(['mean', 'std']).reset_index()
                    df_full = df_full.merge(demand.rename(columns={'mean': 'demand_mean', 'std': 'demand_std'}),
                                          left_on='obj_id', right_on='primary_target_id', how='left')
                
                # Price
                price_mask = df_events['event_type'] == 'PRICE'
                if price_mask.any():
                    latest_price = df_events[price_mask].sort_values('timestamp').drop_duplicates('primary_target_id', keep='last')
                    df_full = df_full.merge(latest_price[['primary_target_id', 'value']].rename(columns={'value': 'price'}),
                                          left_on='obj_id', right_on='primary_target_id', how='left')

            # Fill NaNs
            defaults = {'inventory': 0, 'demand_mean': 0, 'demand_std': 0, 'price': 50.0, 'cost': 25.0, 'moq': 100}
            for col, val in defaults.items():
                if col not in df_full.columns: df_full[col] = val
                else: df_full[col] = df_full[col].fillna(val)
                
            return df_full

    # ==============================================================================
    # âš¡ CORE 2: PLANSMART AGGREGATOR (The Heavy Lifter)
    # ==============================================================================
    
    @lru_cache(maxsize=2)
    def get_planning_tree(self, hierarchy_config_json: str, start_year: int = 2025, horizon_years: int = 1):
        """
        Builds the full 5-Year Hierarchical Data Cube.
        Input: JSON string of hierarchy levels ['Division', 'Category']
        Output: Nested PlanNode structure.
        """
        try:
            levels = json.loads(hierarchy_config_json)
        except:
            levels = ["Division", "Category"] # Fallback

        with sqlite3.connect(self.db_path) as conn:
            # 1. Fetch ALL Data (5 Years)
            
            # A. Products & Hierarchy
            df_prods = pd.read_sql("SELECT obj_id, name, attributes FROM universal_objects WHERE obj_type='PRODUCT'", conn)
            # Expand attributes for grouping
            prod_attrs = pd.DataFrame(df_prods['attributes'].apply(json.loads).tolist())
            df_prods = pd.concat([df_prods[['obj_id', 'name']], prod_attrs], axis=1)
            
            # B. Events
            query = """
                SELECT primary_target_id, event_type, value, timestamp, meta 
                FROM universal_events 
                WHERE event_type IN ('SALES_QTY', 'PRICE', 'RECEIPTS_QTY', 'INV_SNAPSHOT')
            """
            df_events = pd.read_sql(query, conn)
            
            if df_events.empty: return []

            # 2. Pre-Process Events
            df_events['version'] = df_events['meta'].apply(lambda x: json.loads(x).get('version', 'ACTUAL') if x else 'ACTUAL')
            
            # Create Time Buckets (YYYY-MM)
            df_events['date'] = pd.to_datetime(df_events['timestamp'])
            df_events['period'] = df_events['date'].dt.to_period('M').astype(str) # "2025-01"
            
            # 3. Create 'LY' Version (Time Shift Logic)
            # Filter ACTUALs, shift date forward 1 year, label as LY
            df_actuals = df_events[df_events['version'] == 'ACTUAL'].copy()
            df_actuals['date'] = df_actuals['date'] + pd.DateOffset(years=1)
            df_actuals['period'] = df_actuals['date'].dt.to_period('M').astype(str)
            df_actuals['version'] = 'LY'
            
            # Combine Original + Shifted LY
            df_combined = pd.concat([df_events, df_actuals])
            
            # 4. Join Product Hierarchy
            df_merged = df_combined.merge(df_prods, left_on='primary_target_id', right_on='obj_id', how='inner')
            
            # 5. Recursive Tree Builder
            return self._build_recursive_nodes(df_merged, levels, 0, None)

    def _build_recursive_nodes(self, df, levels, current_depth, parent_id):
        if current_depth >= len(levels): return []
        
        level_name = levels[current_depth]
        # Group by current level (e.g., "Division")
        # If level doesn't exist in columns, group by 'name' (leaf)
        group_col = level_name.lower() if level_name.lower() in df.columns else 'name'
        
        # Handle case where level column might be missing
        if group_col not in df.columns:
            group_col = 'name' # Fallback to leaf level aggregation

        grouped = df.groupby(group_col)
        nodes = []
        
        for group_key, group_df in grouped:
            node_id = f"{level_name[:3]}-{str(group_key).replace(' ', '')}"
            
            # --- AGGREGATE DATA FOR THIS NODE ---
            pivot = group_df.pivot_table(
                index='period', 
                columns=['version', 'event_type'], 
                values='value', 
                aggfunc='sum'
            ).fillna(0)
            
            # Convert Pivot to "PlanSmart" Data Structure
            node_data = {}
            for period in pivot.index:
                period_data = {
                    "periodId": period,
                    "sales_units": {}, "sales_amt": {}, "aur": {}, "receipts": {}, "inventory": {}
                }
                
                for ver in ['WP', 'OP', 'LY', 'ACTUAL']:
                    # Extract Metrics
                    units = pivot.loc[period, (ver, 'SALES_QTY')] if (ver, 'SALES_QTY') in pivot.columns else 0
                    price_sum = pivot.loc[period, (ver, 'PRICE')] if (ver, 'PRICE') in pivot.columns else 0
                    rec = pivot.loc[period, (ver, 'RECEIPTS_QTY')] if (ver, 'RECEIPTS_QTY') in pivot.columns else 0
                    inv = pivot.loc[period, (ver, 'INV_SNAPSHOT')] if (ver, 'INV_SNAPSHOT') in pivot.columns else 0
                    
                    sales_amt = units * 45.0 # Placeholder heuristic
                    aur = sales_amt / units if units > 0 else 0

                    period_data['sales_units'][ver.lower()] = {"value": int(units), "locked": False}
                    period_data['sales_amt'][ver.lower()] = {"value": int(sales_amt), "locked": False}
                    period_data['aur'][ver.lower()] = {"value": round(aur, 2), "locked": False}
                    period_data['receipts'][ver.lower()] = {"value": int(rec), "locked": False}
                    period_data['inventory'][ver.lower()] = {"value": int(inv), "locked": False}

                node_data[period] = period_data

            # Recursion
            children = self._build_recursive_nodes(group_df, levels, current_depth + 1, node_id)
            
            nodes.append({
                "id": node_id,
                "name": str(group_key),
                "level": level_name,
                "data": node_data,
                "children": children,
                "isExpanded": current_depth < 1 # Auto-expand top level
            })
            
        return nodes

    # ==============================================================================
    # 3. TACTICAL ENGINES (Optimized)
    # ==============================================================================
    
    def run_replenishment_simulation(self) -> List[Dict]:
        df = self._hydrate_context()
        results = []
        for _, row in df.iterrows():
            # Logic: Simple Safety Stock
            demand = row['demand_mean']
            inv = row['inventory']
            target = demand * 30 # 30 Days Cover
            rec = max(0, target - inv)
            
            decision = "ORDER" if rec > 0 else "HOLD"
            rationale = f"Cover: {(inv/demand if demand>0 else 99):.1f} Days"
            
            # Profit Check
            if rec > 0 and self.validator:
                val = self.validator.validate("BUY", rec * row['price'], rec * row['cost'], 60)
                if not val['approved']: 
                    decision = "DEBATE"
                    rationale = val['reason']

            results.append({
                "node_id": row['obj_id'], "name": row['name'], "current_inv": inv,
                "recommendation": round(rec, 0), "decision": decision, "rationale": rationale
            })
        return results

    def run_pricing_simulation(self) -> List[Dict]:
        df = self._hydrate_context()
        results = []
        for _, row in df.iterrows():
            price = row['price']
            # Elasticity Logic
            new_price = price * 1.05
            decision = "HOLD"
            rationale = "Market Stable"
            
            if self.validator:
                # Sim: 5% price hike, 3% vol drop
                revenue_impact = (new_price * 0.97) - price
                val = self.validator.validate("PRICE", revenue_impact, 0, 30)
                if val['approved']:
                    decision = "HIKE"
                    rationale = "Elasticity Accretive"
            
            if decision != "HOLD":
                results.append({
                    "node_id": row['obj_id'], "name": row['name'], "current_inv": f"${price}",
                    "recommendation": round(new_price, 2), "decision": decision, "rationale": rationale
                })
        return results

    def run_markdown_simulation(self) -> List[Dict]:
        df = self._hydrate_context()
        results = []
        for _, row in df.iterrows():
            inv = row['inventory']
            if inv > 100: # High stock
                new_price = row['price'] * 0.8
                cost_of_carry = inv * row['cost'] * 0.10 # 10% holding cost
                margin_loss = (row['price'] - new_price) * inv
                
                decision = "MARKDOWN" if margin_loss < cost_of_carry else "HOLD"
                rationale = "Liquidation > Holding Cost" if decision == "MARKDOWN" else "Keep Full Price"
                
                results.append({
                    "node_id": row['obj_id'], "name": row['name'], "current_inv": inv,
                    "recommendation": new_price, "decision": decision, "rationale": rationale
                })
        return results

    def run_assortment_simulation(self) -> List[Dict]:
        df = self._hydrate_context(days_history=90)
        results = []
        for _, row in df.iterrows():
            sales = row['demand_mean'] * 90
            margin = (row['price'] - row['cost']) * sales
            holding = (row['inventory'] * row['cost']) * 0.05
            npv = margin - holding
            
            decision = "KEEP"
            if npv < 0: decision = "DROP"
            elif sales > 500: decision = "EXPAND"
            
            results.append({
                "skuId": row['obj_id'], "name": row['name'],
                "velocity": "High" if sales > 200 else "Low",
                "decision": decision, "rationale": f"NPV ${npv:.0f}",
                "metrics": {"npv": npv}
            })
        return results

    def run_allocation_simulation(self) -> List[Dict]:
        # Mock Allocation using Hydrated Data
        df = self._hydrate_context()
        results = []
        candidates = df[df['inventory'] > 50].head(10) # Sources
        for _, row in candidates.iterrows():
            results.append({
                "id": f"TR-{row['obj_id'][:4]}",
                "sku": row['name'],
                "source": "DC_WEST", "dest": "STORE_NYC",
                "items": 20, "status": "Standard", "decision": "APPROVED",
                "rationale": "Balancing Stocks"
            })
        return results

    # --- SYSTEM 3 INTERFACE ---
    def convene_council_for_node(self, node_id: str, mode: str) -> Dict[str, Any]:
        if not debate_engine: return {"decision": "OFFLINE", "transcript": []}
        return debate_engine.convene_council({"name": node_id}, mode=mode)

# Singleton Instance
orchestrator = Orchestrator()
